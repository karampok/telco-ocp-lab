---
run-name: ztp-compact-${{ github.ref_name }}-${{ github.event_name }}
name: ztp-compact
on:  # yamllint disable-line rule:truthy
  push:
    branches:
      - dev
      - main
  schedule:
    - cron: '0 */3 * * *'
  workflow_dispatch:
jobs:
  stage0:
    name: ZTP E2E
    runs-on: [self-hosted, green]
    outputs:
      RUNNER: ${{ runner.name }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Clean Virtual Infra
        run: |
          ./sinfra --clean -auto-timeout 0s --continue-on-error
      - name: Setup Virtual Infra - ${{ runner.name }}
        run: |
          echo '${{ secrets.PULL_SECRET }}' > .pull-secret.json
          echo '${{ secrets.WG_AUTH }}' > .wg-auth
          ./sinfra --setupInfra -auto-timeout 0s
          virsh dumpxml 5gc-w0|grep emulator
      - name: Reserve Runner
        run: |
          podman exec workstation bash -ce '
            GH_TOKEN='${{ secrets.GH_TOKEN }}' ./bin/label-runner.sh ${{ runner.name }} -green +taken
          '
  stage1:
    name: Deploy SNO Cluster - ${{needs.stage0.outputs.RUNNER}}
    runs-on: ${{needs.stage0.outputs.RUNNER}}
    needs: [stage0]
    if: needs.stage0.result == 'success'
    steps:
      - name: Deploy SNO using agent-based - OCP 4.14.1 / gitops 1.9
        timeout-minutes: 40
        run: |
          podman exec workstation bash -ce '
            curl https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.14.1/openshift-install-linux-4.14.1.tar.gz -o openshift-install-linux.tar.gz
            tar xvfz openshift-install-linux.tar.gz
            mv openshift-install /usr/bin/
            chmod +x /usr/bin/openshift-install

            ./deploy-ocp.sh hub

            export KUBECONFIG=/share/hub/auth/kubeconfig
            openshift-install agent wait-for install-complete --log-level info --dir /share/hub
            oc get clusterversion && oc get co && oc get ns openshift-gitops || true

            echo "${{ secrets.CI_SSH_KEY }}" > ~/.ssh/id_rsa && chmod 600 ~/.ssh/id_rsa
          '
          echo "wget http://10.0.0.1:9000/hub/auth/kubeconfig -O ~/.kube/hub.yaml"
          echo "curl -s http://10.0.0.1:9000/hub/auth/kubeadmin-\password | xsel --input --clipboard"
  stage1a:
    name: Install Gitops Operator   - ${{needs.stage0.outputs.RUNNER}}
    runs-on: ${{needs.stage0.outputs.RUNNER}}
    needs: [stage0, stage1]
    if: needs.stage1.result == 'success'
    steps:
      - name: Deploy SNO using agent-based - OCP 4.14.1 / gitops 1.9
        timeout-minutes: 40
        run: |
          podman exec workstation bash -ce '
            oc get ns openshift-gitops
            oc -n openshift-gitops get routes openshift-gitops-server ||  oc -n openshift-operators delete pods --all
          '
  stage2:
    name: Provision HUB Cluster
    runs-on: ${{needs.stage0.outputs.RUNNER}}
    needs: [stage0, stage1a]
    if: needs.stage1.result == 'success'
    steps:
      - name: Install ACM / LVM / TALM using gitops - release-2.8 / stable-4.1 / stable
        timeout-minutes: 10
        run: |
          podman exec workstation bash -ce '
            until oc get Application 2>/dev/null; do sleep 30; done
            oc apply -f hub/hub-provision-argoapp.yaml
          '
      - name: Install ZTP - point to ${{ github.sha }}
        run: |
          podman exec workstation bash -ce '
            BRANCH=${{ github.sha }} PULL_SECRET_PATH=.pull-secret.json ./hub/install-ztp.sh
          '
      - name: Wait HUB to be ready - OCP 4.14.1
        run: |
          podman exec workstation bash -ce '
            sleep 240
            until oc get MultiClusterHub 2>/dev/null; do sleep 30; done
            until oc get LVMCluster 2>/dev/null; do sleep 30; done
            until oc -n openshift-storage get lvmcluster 2>/dev/null; do sleep 30; done
            oc -n openshift-storage wait lvmcluster lvmcluster --for=jsonpath='{.status.state}'=Ready --timeout=1200s
            oc -n open-cluster-management wait MultiClusterHub multiclusterhub --for=jsonpath='{.status.phase}'=Running --timeout=1200s
          '
      - name: Review argocd sync
        continue-on-error: true
        run: |
          podman exec workstation bash -ce '
            pass=$(oc get secret -n openshift-gitops openshift-gitops-cluster -o jsonpath="{.data.admin\.password}" |base64 -d)
            argocd login openshift-gitops-server-openshift-gitops.apps.hub.eric.vlab --username admin --insecure --password $pass
            argocd app list
            argocd app get hub-provision
            argocd app get clusters
            argocd app get policies
          '
  stage3:
    name: Wait SPOKE
    runs-on: ${{needs.stage0.outputs.RUNNER}}
    needs: [stage0, stage2]
    if: needs.stage2.result == 'success'
    steps:
      - name: Enable autosecrets hack
        run: |
          echo '${{ secrets.BMH_SECRET }}' > bmh-secret.yaml
          podman exec workstation bash -ce '
            oc apply -f bmh-secret.yaml
            oc apply -k github.com/karampok/autosecret/config/default
            echo "https://openshift-gitops-server-openshift-gitops.apps.hub.eric.vlab/"
          '
      - name: Wait Siteconfig
        timeout-minutes: 150
        run: |
          podman exec workstation bash -ce '
            sleep 1200
            timeout 540 ./bin/watch-ztp 5gc || echo
            oc -n 5gc wait clusterdeployment 5gc --for=condition=Provisioned --timeout=5400s ||true
            timeout 10 ./bin/watch-ztp 5gc || echo
            oc -n 5gc wait clusterdeployment 5gc --for=condition=Provisioned --timeout=5400s
          '
      - name: Access spoke cluster
        run: |
          podman exec workstation bash -ce '
            oc extract secret/5gc-admin-kubeconfig --to=- -n 5gc > /tmp/spoke.yaml
            export KUBECONFIG=/tmp/spoke.yaml
            oc get clusterversion
            oc get nodes
            oc get co
          '
      - name: Wait Policies
        timeout-minutes: 120
        run: |
          podman exec workstation bash -ce '
            sleep 1200
            oc get cgu -A
            timeout 540 oc -n 5gc get policies -w || echo ""
            oc get policies -A
            oc -n ztp-install wait cgu 5gc --for=condition=Succeeded --timeout=5400s
            oc -n 5gc get policies
            oc -n ztp-install get cgu 5gc -o yaml
          '
  stage4:
    name: Test SPOKE
    runs-on: ${{needs.stage0.outputs.RUNNER}}
    needs: [stage0, stage3]
    if: needs.stage3.result == 'success'
    steps:
      - name: Test Metallb setup
        run: |
          podman exec workstation bash -cex '
            oc extract secret/5gc-admin-kubeconfig --to=- -n 5gc > /tmp/spoke.yaml
            export KUBECONFIG=/tmp/spoke.yaml

            oc apply -f dayX/macnet0-bridge.yaml
            oc apply -f dayX/macnet1-private.yaml
            oc apply -f dayX/red-net.yaml
            oc apply -f dayX/icmp-frag-case.yaml

            sleep 30

            # configmap to enable bgp learning should have been
            # before establishing the BGP sessiong, FRR limitation
            oc -n metallb-system delete pods -l app=metallb,component=speaker

            oc wait --for=condition=ready pod -l app=macvlan --timeout=320s
            oc wait --for=condition=ready pod -l app=bigfile --timeout=320s
            oc wait --for=condition=ready pod -l app=rednet --timeout=320s

            oc get nodes || true
            oc get pods || true
            oc get svc || true
          '
          podman exec green bash -cex '
            echo "from green client to access the service in green segment (static)"
            wget --timeout=1 --tries=8 5.8.6.9:9000/big.iso
            wget --timeout=1 --tries=8 5.8.6.1:9000/big.iso
          '
          podman exec red bash -cex '
            echo "from red client to access the service in red segment (bgp learning)"
            curl --retry 5 -s http://6.6.6.1/
          '
        continue-on-error: false
      - name: Test macvlan secondary interfaces
        run: |
          podman exec macnet bash -cex '
            echo "macnet to access pods over secondary interface"
            # test bridge mode access
            nc -vz 172.100.125.100 1111
            nc -vz 172.100.125.200 2222
            # test private private access
            nc -vz 172.100.127.100 1111
            nc -vz 172.100.127.110 1111
            nc -vz 172.100.127.200 2222
          '
        continue-on-error: false
      - name: Test DPDK setup
        run: |
          podman exec workstation bash -ce '
            oc extract secret/5gc-admin-kubeconfig --to=- -n 5gc > /tmp/spoke.yaml
            export KUBECONFIG=/tmp/spoke.yaml

            oc apply -f dayX/dpdk-pod.yaml
            oc wait --for=condition=ready pod dpdk --timeout=320s

            sleep 30

            oc get nodes || true
            oc get pods || true
          '
      - name: Mark Runner
        run: |
          podman exec workstation bash -ce '
            GH_TOKEN='${{ secrets.GH_TOKEN }}' ./bin/label-runner.sh ${{ runner.name }} +ready -taken
          '

# To add github action runner
# https://github.com/karampok/telco-ocp-lab/settings/actions/runners/new
#  mkdir actions-runner && cd actions-runner
#  curl -o actions-runner-linux-x64-2.309.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.309.0/actions-runner-linux-x64-2.309.0.tar.gz
#  tar xzf ./actions-runner-linux-x64-2.309.0.tar.gz
#  ./bin/installdependencies.sh
#  export RUNNER_ALLOW_RUNASROOT=1
#  ./config.sh --url https://github.com/karampok/telco-ocp-lab --token XXXX
#  dnf install tmux
#  tmux new-session -d ./run.sh
