---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "htworker"
  namespace: "ztp-group"
spec:
  bindingRules:
    htworker: ""
  remediationAction: inform
  mcp: "ht100gb"
  sourceFiles:
    # configure SR-IOV
    - fileName: SriovSupportedNicCM.yaml
      policyName: "patch-sriov-cm-hack-w3"
    - fileName: SriovOperatorConfig.yaml
      policyName: "config-operator-sriov-w10"
      spec:
        enableOperatorWebhook: false
        logLevel: 0
    - fileName: SriovNetwork.yaml
      policyName: config-sriov-sn-w100
      metadata:
        name: "leftnet"
      spec:
        resourceName: sriovleftdpdk
    - fileName: SriovNetwork.yaml
      policyName: config-sriov-sn-w100
      metadata:
        name: "rightnet"
      spec:
        resourceName: sriovrightdpdk
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: config-sriov-snnp-w100
      metadata:
        name: sriovleftdpdk
      spec:
        resourceName: sriovleftdpdk
        nodeSelector:
          node-role.kubernetes.io/ht100gb: ""
        numVfs: 4
        priority: 99
        nicSelector:
          pfNames: ['enp41s0']
        deviceType: vfio-pci
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: config-sriov-snnp-w100
      metadata:
        name: sriovrightdpdk
      spec:
        resourceName: sriovrightdpdk
        nodeSelector:
          node-role.kubernetes.io/ht100gb: ""
        numVfs: 4
        priority: 99
        nicSelector:
          pfNames: ['enp42s0']
        deviceType: vfio-pci
    # configure MachineConfigPools
    - fileName: MachineConfigPool.yaml
      policyName: config-nodes-w1
      metadata:
        name: ht100gb
        annotations:
          ran.openshift.io/ztp-deploy-wave: "1"
        labels:
          machineconfiguration.openshift.io/role: ht100gb
          pools.operator.machineconfiguration.openshift.io/ht100gb: ""
      spec:
        machineConfigSelector:
          matchExpressions:
            - key: machineconfiguration.openshift.io/role
              operator: In
              values: [worker, ht100gb]
        nodeSelector:
          matchLabels:
            node-role.kubernetes.io/ht100gb: ""
        paused: false
    # configure NUMAResource
    - fileName: NUMAResourcesOperator/instance.yaml
      policyName: config-ht-node
    - fileName: PerformanceProfile.yaml
      policyName: config-ht-node
      complianceType: mustonlyhave
      metadata:
        name: ht100gb
        annotations:
          kubeletconfig.experimental: |
            {"systemReserved": {"memory": "2Gi"}, "topologyManagerScope": "pod"}
      spec:
        additionalKernelArgs: []
        cpu:
          isolated: "4-23"
          reserved: "0-3"
        hugepages:
          defaultHugepagesSize: 1G
          pages:
            - size: 1G
              count: 3
        numa:
          topologyPolicy: single-numa-node
        workloadHints:
          realTime: false
          highPowerConsumption: false
          perPodPowerManagement: false
        realTimeKernel:
          enabled: false
